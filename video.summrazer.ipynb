{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# video.summrazer.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the video from Youtube\n",
    "First of all, we need a way to download the video from youtube. Actually, we don’t need the whole video but only the audio. So we will extract the audio from the video and download only that.\n",
    "\n",
    "The library I used to interoperate with youtube is youtube_dl which you can learn more about on GitHub.\n",
    "\n",
    "So we install the library with pip and download the audio from youtube in the following way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Audio extracted and saved to output_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "# Specify the path to your local video file\n",
    "local_video_path = \"bigo.mp4\"\n",
    "\n",
    "# Load the video clip\n",
    "video_clip = VideoFileClip(local_video_path)\n",
    "\n",
    "# Extract audio from the video clip\n",
    "audio_clip = video_clip.audio\n",
    "\n",
    "# Specify the output audio file name\n",
    "output_audio_path = \"output_audio.wav\"\n",
    "\n",
    "# Write the audio to a WAV file\n",
    "audio_clip.write_audiofile(output_audio_path)\n",
    "\n",
    "# Close the video clip to release resources\n",
    "video_clip.close()\n",
    "\n",
    "print(f\"Audio extracted and saved to {output_audio_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in line 8 we chose to download the audio in wav format, but mp3 or others are also fine if you prefer.\n",
    "In line 15, on the other hand, you must enter the link to the video you want."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Listen to the Audio\n",
    "Did we download the audio correctly? Let’s check by resenting the audio directly from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pocketsphinx in /usr/local/python/3.10.13/lib/python3.10/site-packages (5.0.3)\n",
      "Requirement already satisfied: sounddevice in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pocketsphinx) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in /home/codespace/.local/lib/python3.10/site-packages (from sounddevice->pocketsphinx) (1.16.0)\n",
      "Requirement already satisfied: pycparser in /home/codespace/.local/lib/python3.10/site-packages (from CFFI>=1.0->sounddevice->pocketsphinx) (2.21)\n"
     ]
    }
   ],
   "source": [
    "!pip install pocketsphinx\n",
    "\n",
    "import speech_recognition as sr\n",
    "\n",
    "# Define the path to your WAV file\n",
    "wav_file = \"output_audio.wav\"\n",
    "\n",
    "# Initialize the recognizer\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "# Load the audio file\n",
    "with sr.AudioFile(wav_file) as source:\n",
    "    # Listen for the data (load audio to memory)\n",
    "    audio_data = recognizer.record(source)\n",
    "    \n",
    "    # Recognize speech using the Sphinx engine (offline)\n",
    "    try:\n",
    "        text = recognizer.recognize_sphinx(audio_data)\n",
    "        print(\"Extracted text:\", text)\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sphinx could not understand the audio\")\n",
    "    except sr.RequestError as e:\n",
    "        print(\"Sphinx error; {0}\".format(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Audio to Text\n",
    "The next step is to convert the audio file into text hoping to get a low word error rate. This will be useful since we can then run a summarization NLP algorithm directly on the text.\n",
    "\n",
    "You can read more about the model we will use for text conversion to text here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "model = \"facebook/wav2vec2-large-960h-lv60-self\" #speech to text\n",
    "\n",
    "#speech to text\n",
    "pipe = pipeline(model = model)\n",
    "text = pipe('output_audio.wav', chunk_length_s=10) \n",
    "\n",
    "#save text\n",
    "text_file = open(\"original_text.txt\", \"w\")\n",
    "n = text_file.write(text[\"text\"])\n",
    "text_file.close()\n",
    "\n",
    "#read article\n",
    "text_article = open(\"original_text.txt\", \"r\").read()\n",
    "print(len(text_article.split()))\n",
    "text_article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Summarization\n",
    "Now what’s left for us to do is to take the text we extracted from the video and summarize it.\n",
    "There are hundreds of summarization models, all you have to do is go to hugging face filter on the summarization button and choose the one that best suits your case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project, I will use the google/pegasus-xsum model. You can read the details of this model here (in some future articles I will also go on to explain the theory behind these summarization algorithms).\n",
    "\n",
    "Using these pre-trained models found on HugginFace is really simple, look at I use summarization in a few lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 35\u001b[0m\n\u001b[1;32m     33\u001b[0m output_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msummary.txt\u001b[39m\u001b[38;5;124m'\u001b[39m  \u001b[38;5;66;03m# Replace with the desired path for the output summary .txt file\u001b[39;00m\n\u001b[1;32m     34\u001b[0m sentences_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m  \u001b[38;5;66;03m# Adjust the number of sentences in the summary as needed\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m \u001b[43msummarize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences_count\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 25\u001b[0m, in \u001b[0;36msummarize_text\u001b[0;34m(input_file, output_file, sentences_count)\u001b[0m\n\u001b[1;32m     22\u001b[0m parser \u001b[38;5;241m=\u001b[39m PlaintextParser\u001b[38;5;241m.\u001b[39mfrom_string(text, Tokenizer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menglish\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     23\u001b[0m summarizer \u001b[38;5;241m=\u001b[39m LexRankSummarizer()\n\u001b[0;32m---> 25\u001b[0m summary \u001b[38;5;241m=\u001b[39m \u001b[43msummarizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msentences_count\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m summary_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m summary])\n\u001b[1;32m     28\u001b[0m write_summary(summary_text, output_file)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sumy/summarizers/lex_rank.py:41\u001b[0m, in \u001b[0;36mLexRankSummarizer.__call__\u001b[0;34m(self, document, sentences_count)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[1;32m     40\u001b[0m tf_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_tf(sentences_words)\n\u001b[0;32m---> 41\u001b[0m idf_metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentences_words\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_matrix(sentences_words, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mthreshold, tf_metrics, idf_metrics)\n\u001b[1;32m     44\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpower_method(matrix, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepsilon)\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sumy/summarizers/lex_rank.py:85\u001b[0m, in \u001b[0;36mLexRankSummarizer._compute_idf\u001b[0;34m(sentences)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m idf_metrics:\n\u001b[0;32m---> 85\u001b[0m             n_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msentences\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mterm\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m             idf_metrics[term] \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(sentences_count \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m n_j))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idf_metrics\n",
      "File \u001b[0;32m/usr/local/python/3.10.13/lib/python3.10/site-packages/sumy/summarizers/lex_rank.py:85\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m sentence:\n\u001b[1;32m     84\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m idf_metrics:\n\u001b[0;32m---> 85\u001b[0m             n_j \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mif\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m s)\n\u001b[1;32m     86\u001b[0m             idf_metrics[term] \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mlog(sentences_count \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m n_j))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m idf_metrics\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer\n",
    "\n",
    "def read_text(file_path,encoding='latin1'):\n",
    "    \"\"\"Reads text from a file.\"\"\"\n",
    "    with open(file_path, 'r',encoding=encoding) as file:\n",
    "        return file.read()\n",
    "\n",
    "def write_summary(summary, output_file):\n",
    "    \"\"\"Writes summary to a file.\"\"\"\n",
    "    with open(output_file, 'w') as file:\n",
    "        file.write(summary)\n",
    "\n",
    "def summarize_text(input_file, output_file, sentences_count=3):\n",
    "    \"\"\"Summarizes text using `sumy`.\"\"\"\n",
    "    text = read_text(input_file)\n",
    "\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(\"english\"))\n",
    "    summarizer = LexRankSummarizer()\n",
    "\n",
    "    summary = summarizer(parser.document, sentences_count)\n",
    "    summary_text = '\\n'.join([str(sentence) for sentence in summary])\n",
    "\n",
    "    write_summary(summary_text, output_file)\n",
    "    print(f\"Summary written to {output_file}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    input_file = 'output_audio.wav'  # Replace with the path to your input .txt file\n",
    "    output_file = 'summary.txt'  # Replace with the desired path for the output summary .txt file\n",
    "    sentences_count = 3  # Adjust the number of sentences in the summary as needed\n",
    "    summarize_text(input_file, output_file, sentences_count)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](image-1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
